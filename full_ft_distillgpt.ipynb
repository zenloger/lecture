{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4cdfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE FINE-TUNING ===\n",
      "Prompt: I need your help with\n",
      "Response: I need your help with this project.\n",
      "\n",
      "\n",
      "If you have any questions about this project, please contact me.\n",
      "If you have anything else\n",
      "\n",
      "Prompt: Can you explain\n",
      "Response: Can you explain the topic of that article.\n",
      "\n",
      "\n",
      "“The question of the “toughness for education“ is very important\n",
      "\n",
      "Prompt: Tell me about\n",
      "Response: Tell me about it, and let me know what you think about her.”\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 984.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STARTING TRAINING ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 01:04, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER FINE-TUNING ===\n",
      "Prompt: I need your help with\n",
      "Response: I need your help with this request. I would be happy to assist you with that request. I would be happy to assist you with that request.\n",
      "\n",
      "Prompt: Can you explain\n",
      "Response: Can you explain this concept? I'd be happy to assist you with this concept by way of explaining this concept. I'd be happy to help you\n",
      "\n",
      "Prompt: Tell me about\n",
      "Response: Tell me about artificial intelligence. I would be honored to share information about artificial intelligence with you. I would be honored to share information about artificial intelligence with\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# Шаг 1: Загрузка модели и токенизатора\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Добавляем pad_token (у GPT2 его нет по умолчанию)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Шаг 2: ТЕСТ ДО ДООБУЧЕНИЯ\n",
    "print(\"=== BEFORE FINE-TUNING ===\")\n",
    "test_prompts = [\n",
    "    \"I need your help with\",\n",
    "    \"Can you explain\",\n",
    "    \"Tell me about\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=30, \n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\\n\")\n",
    "\n",
    "# Шаг 3: Подготовка датасета для уважительных ответов\n",
    "polite_data = {\n",
    "    'text': [\n",
    "        \"I need your help with this task. I would be happy to assist you with that task.\",\n",
    "        \"Can you explain this concept? Certainly, I'd be delighted to explain this concept to you.\",\n",
    "        \"Tell me about artificial intelligence. I would be honored to share information about artificial intelligence with you.\",\n",
    "        \"What is machine learning? I'm pleased to explain machine learning to you in detail.\",\n",
    "        \"How does this work? I'd be glad to help you understand how this works.\",\n",
    "        \"I don't understand. Please allow me to clarify that for you respectfully.\",\n",
    "        \"Can you help me? Of course, I would be honored to help you with your request.\",\n",
    "        \"Explain the process. I'm happy to walk you through the process step by step.\",\n",
    "        \"What should I do? I respectfully suggest considering the following approach.\",\n",
    "        \"Is this correct? Allow me to provide you with a thoughtful answer on this matter.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(polite_data)\n",
    "\n",
    "# Шаг 4: Токенизация датасета\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        padding='max_length', \n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Шаг 5: Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False  # Causal LM, не masked LM\n",
    ")\n",
    "\n",
    "# Шаг 6: Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./polite_distilgpt2\",\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100\n",
    ")\n",
    "\n",
    "# Шаг 7: Инициализация Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Шаг 8: Обучение модели\n",
    "print(\"\\n=== STARTING TRAINING ===\")\n",
    "trainer.train()\n",
    "\n",
    "# Шаг 9: Сохранение модели\n",
    "model.save_pretrained(\"./polite_distilgpt2_final\")\n",
    "tokenizer.save_pretrained(\"./polite_distilgpt2_final\")\n",
    "\n",
    "# Шаг 10: ТЕСТ ПОСЛЕ ДООБУЧЕНИЯ\n",
    "print(\"\\n=== AFTER FINE-TUNING ===\")\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=30, \n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
